---
layout: post
title:  "ReproducibiliTea does RepliCATS"
date:   2020-02-22 11:51:00 +0000
author: Matt Jaquiery
categories: [JC]
tags: [Oxford, RepliCATS]
---

{% include image.html url="blog/replicats-oxford.jpg" alt="ReproducibiliTea Oxford does RepliCATS" description="<a href='/journal-clubs/#Oxford'>ReproducibiliTea Oxford</a> RepliCATS session. Photo credit <a href='https://twitter.com/IlsePit'>@IlsePitt</a>" %}

Last week's ReproducibiliTea at [Oxford Experimental Psychology](/journal-clubs/#Oxford) (honourable mention to our Anthropology regulars!) went on a little longer than the usual one-hour snacks and journal club format. We, like a few other ReproducibiliTea chapters, teamed up with the [RepliCATS](https://replicats.research.unimelb.edu.au/) projects to produce some estimates of research replicability for science (and pizza).

RepliCATS ([@replicats](https://twitter.com/replicats)) is a project aiming to provide insights into the replicability of science and how well it can be predicted. There are three arms to the project: gathering researchers' assessments of replicability of specific claims in papers across a broad range of disciplines, running some replications of a subset of the claims assessed, and attempting to predict replicability using machine learning techniques. The ReproducibiliTea sessions are helping with the first of these.

After half an hour or so of signing up to the platform (sign-up involves an interesting questionnaire about areas of expertise, metascience knowledge, and statistical knowledge) and watching the intro video, we spent a couple of hours going over four claims using the platform's IDEA approach. As a graduate student studying group decision-making, it was pleasing to see the IDEA protocol is based on group decision-making literature: individuals make their own judgements, then have an opportunity to discuss those judgements with others, before each person makes a final decision (in private). This allows for both a diversity of estimates and reasons as well as minimising groupthink effects. At least in principle - as a group many of us quite effusive (especially me) so there may have been some cross-pollination of ‘individual judgements’ due to facial expressions, quizzical sounds, or irrepressible comments.
 
 Each claim is judged on how readily it can be understood, whether it is prima facie plausible, and how many of 100 direct replications of the effect are likely to show an effect in the same direction (with alpha = .05). This seems pretty straightforward, but in practice it often turned out to be quite tricky...

For some of the claims we found that the claim extracted by the platform for assessment bore little resemblance to the inferential test ostensibly put forward to test it. In one example there was a claim that watching short clips of orchestras playing allowed better judgement than watching + listening to those clips or simply listening alone, and this claim was ‘tested’ by a t-test of discrimination rate for watching vs. chance. Sometimes this discrepancy was because the paper didn't do a good job of testing the claim, but in others it was because the wrong test was selected by the platform. Additionally, in the orchestra example, as well as some others, the N supplied for the test didn’t match the actual N used (derived from reading the paper + observing the test degrees of freedom). Those issues made it difficult to work out whether we were assessing the replicability of the claim or of the inferential test.

For other claims the tests seemed entirely misguided – one paper seemed to be dividing teachers into two groups based on whether their teaching was improving, and then used an inferential test to show that the ‘improving’ group improved relative to the non-improving group. That test seemed pretty pointless in terms of a meaningful claim, and it wasn’t very clear if we should be assessing the probability that for any random set of 6 teachers you’ll be able to assign them to improving vs stable groups, or whether we should be assessing the claim that tautological t-tests should come up significant.

Of the four claims we investigated as a group, the most sensible one was to do with juvenile offenders being more likely to be placed in young offenders’ institutions if their family was assessed as dysfunctional. We were pretty happy with the inferential test for that claim, and the only reason we could come up with to doubt the replicability (it seems like an intuitive claim) was less to do with the robustness of the study than its generalisability across time: it seemed to be showing a behaviour of the courts which could be subject to political correction and thus make it hard to find the same effect because courts where replications were run had been instructed to behave differently (perhaps as a result of the original study). The whole exercise brought to light a good many considerations of what we mean by replicability and which factors we should or should not care about. It was also like a rapid-fire journal club with pizza, which is about the best way to spend an academic afternoon!

I really like the ambition of the whole repliCATS project, from the breadth and depth of the disciplines (including planning replications) covered to the plausibility of running machine learning on the studies from both a paper textual/statistical analysis perspective and an expert-judgement perspective (I think the latter is less likely to be derailed by the black boxiness of machine learning). It’s a very cool project and I’m really looking forward to seeing how it evolves.
 